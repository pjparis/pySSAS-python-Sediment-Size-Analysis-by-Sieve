{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sediment Size Analysis by Sieve (SedSAS) Class : Example\n",
    "\n",
    "### Processing sediment data stored in a Microsoft Excel (xls or xlsx) file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part I: Introduction:\n",
    "\n",
    "SedSAS is a class object written in the Python programming (scripting) language. Its purpose is to provide a basic set of statistical and visualization tools for analyzing unconsolidated sediment size-fraction samples collected in the field and separated using either mechanical sieves or any other analog partition-by-size methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Sample Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we look at how the class is used to computer grain-size statistics for a one or more samples, where subsample weights are contained a separate Microsoft Excel spreadsheet file. The data from the spreadsheet will be read into this notebook and processed.\n",
    "\n",
    "You can use this notebook not only as a learning tool or reference but also as a template from which to directly conduct your own analyses. To do the latter, simply download a copy and replace the existing data file path(s) and name(s), then run.\n",
    "\n",
    "The downloaded notebook can be further modified, or copied and then the copy modified, as you see fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we'll do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Load data from a Microsoft Excel spreadsheet into a Pandas dataframe <br/>\n",
    "2.) Rework the contents of the new Pandas dataframe so as to meet the input and format requirements of SedSAS <br/>\n",
    "3.) Loop thru each sample in the dataframe to compute grain-size statistics <br/>\n",
    "4.) Write the results to a new Pandas dataframe and display the contents in the notebook <br/>\n",
    "5.) Save the contents of the new results dataframe to a new csv file <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:\n",
    "- The SedSAS class, this notebook, and all supporting materials were developed in a Python 3.x environment. Neither the class nor any of these supporting materials have been tested using a Python 2.x distro. *Backward compatibility is expected, but not assured.*\n",
    "    \n",
    "- If you do not have a Python interpreter installed on your computer (from the factory Linux UNIX, and MacOS users do; Windows users likely do not), your best bet for a trouble-free installation experience can be had here: https://anaconda.org/anaconda/python. If you're more adventurous, try the Python Software Foundation: https://www.python.org/getit/. Even if your OS does come with a Python interpreter already in place, it's probably a good idea to get a more up to date release (most OS installs are a version, or more, behind the current release). Again, I suggest the Anaconda distribution. Be sure to get a copy of Python version 3.5 or later.  \n",
    "- Required external Python Libraries and modules\\*\\*:\n",
    "    - sys (basic Python methods for retrieving information from the host operating system)\n",
    "    - numpy (numeric Python library for array and matrix operations)\n",
    "    - pandas (Python Data Analysis library for in-memory data storage and analysis)\n",
    "    - matlotlib (Matrix Plotting Library for plot generation) \n",
    "    \n",
    "    \n",
    "- Loading SedSAS.py into your script: to use the class it must first be loaded into your script or notebook environment. If the class script file is located in the same directory as your analysis script then to load it you need only do the following:\n",
    "    \n",
    "    import  sedSAS\n",
    "    \n",
    "If the SedSAS.py file is located elsewhere on your computer or server you'll have to point Python to its location by amending the local path variable\\*\\*\\*:\n",
    "\n",
    "    import sys\n",
    "    sys.path.append(/full/path/to/directory/where/class/file/is/located/)\n",
    "    \n",
    "For example, if your copy of SedSAS.py is located in the directory: \n",
    "\n",
    "    /Users/Documents/projects/ \n",
    "    \n",
    "enter this into the sys.path.append method as:\n",
    "\n",
    "    import sys\n",
    "    sys.path.append(/Users/Documents/projects/)\n",
    "\n",
    "\n",
    "______________\n",
    "\\*\\*Note that these, and many, many other libraries, are included in the default Anaconda Python distribution.\n",
    "\n",
    "\\*\\*\\* Note that you are not changing your operating system's global PATH variable, only a copy of it that is assigned to the environment tied to your script or notebook. The global PATH variable is not altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Required User Inputs:\n",
    "Initial data input to SedSAS during the instantiation process consists of:\n",
    "\n",
    "1. a listing of all the sieve apertures (in $\\phi$ units) used in the analysis (sorted descending by size)\n",
    "2. the weight of sediment material captured by each sieve \n",
    "\n",
    "These data must be passed to SedSAS inside a Pandas dataframe where the first column contains the aperture sizes in order as in the actual stack and where the second column contains the sediment weight.\n",
    "\n",
    "A unique identifier for the sample can optionally be passed to SedSAS at the time of instantiation. Note that SedSAS doesn't really care if you provide a unique identifier, nor is it particular about how you choose to format the id. The identifier is more for the user and tracking than for code execution and so from a functional point of view, it's not even a requiremment. Nevertheless, it makes sense to provide something that helps to track what's in process, especially if you want to distinguish between samples when multiple samples are run in succession. If you wish to forego an id, the class will assign the default value: '1' as id each time the the class is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part II: Example: using the class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing grain-size statistics from sample data housed in a Microsoft Excel file located in the subdirectory ./Sample_Data/. \n",
    "\n",
    "- The spreadsheet file to be processed is: Sample_Data.xlsx\n",
    "\n",
    "- The sieves used (in stack order) in this example are: -2.25$\\phi$, -1.0$\\phi$, -0.5$\\phi$, 0.0$\\phi$, 0.5$\\phi$, 1.0$\\phi$, 1.5$\\phi$, 2.0$\\phi$, 2.5$\\phi$, 3.0$\\phi$, 3.5$\\phi$, 4.0$\\phi$, and the pan fraction which we'll designate as 5.0$\\phi$\n",
    "\n",
    "- The SedSAS.py script file is located in the same directory as is my (this) Jupyter notebook.\n",
    "\n",
    "All the required data resides in Sample_Data.xlsx, but the format is not yet ready for direct ingest into SedSAS. Thus, we'll perform some minor preliminary adjustments to get the data in the required format. You will probably have to perform similar adjustment operations in order to prepare your own data for analysis. If you should elect to use SedSAS in your work just remember that Google can be your good friend in finding out how to do something with Python, numpy, and/or Pandas dataframes. Don't be afraid to experiment, and learn. \n",
    "\n",
    "Finally, SedSAS is verbose. As you will see, during execution there is potentially going to be a lot of feedback written back to the console (your computer screen). You are advised to at least scan through this, voluminous as it might be, for important warnings, errors, and other information that might have a direct bearing on the integrity of the results. \n",
    "\n",
    "With all that said, let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the contents of the spreadsheet file into a pandas dataframe and reformat as needed for SedSAS ingest:\n",
    "\n",
    "In this particular example some columns from the source file are dropped, it was neceaary to transpose the new dataframe to arrange it so that each column represented a single sample and each row a single weight observation in a sample. For other files, with other data content and formatting a different preparation will be required. The objective, however, is a simple dataframe containing one column that lists all the sieve screen apertures used in your analysis, and another second column that carries the individual subsample weights stopped by each concomitant sieve screen. \n",
    "\n",
    "Note that, as will be demonstrated here, the aperture column can be proxied by the dataframe's index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2.25</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.470</td>\n",
       "      <td>5.09</td>\n",
       "      <td>2.69</td>\n",
       "      <td>4.66</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.00</th>\n",
       "      <td>2.74</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.990</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.50</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>1.16</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.340</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>1.30</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>1.89</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.460</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.50</th>\n",
       "      <td>1.97</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.26</td>\n",
       "      <td>3.140</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <td>3.16</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.29</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.060</td>\n",
       "      <td>3.72</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.97</td>\n",
       "      <td>4.82</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.50</th>\n",
       "      <td>3.92</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.08</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.20</td>\n",
       "      <td>4.950</td>\n",
       "      <td>4.82</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.81</td>\n",
       "      <td>5.80</td>\n",
       "      <td>4.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.00</th>\n",
       "      <td>3.33</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.210</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.50</th>\n",
       "      <td>1.46</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.00</th>\n",
       "      <td>1.07</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.00</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6      7     8     9     10  \\\n",
       "-2.25  2.74  1.16  2.16  3.46  2.61  3.09  1.94  3.470  5.09  2.69  4.66   \n",
       "-1.00  2.74  1.81  1.63  2.66  2.84  2.35  2.35  1.990  1.50  1.31  1.23   \n",
       "-0.50  1.01  1.14  0.98  1.04  1.27  1.40  1.18  1.080  1.08  1.10  0.89   \n",
       " 0.00  1.16  1.22  1.02  1.31  1.61  1.58  1.23  1.340  1.16  1.35  1.12   \n",
       " 0.50  1.30  1.39  1.28  1.40  1.76  1.78  1.54  1.600  1.48  1.61  1.54   \n",
       " 1.00  1.89  2.20  2.14  2.07  2.59  2.55  2.34  2.460  2.36  2.35  2.57   \n",
       " 1.50  1.97  2.29  2.35  1.99  2.45  2.54  2.26  3.140  2.44  2.25  2.54   \n",
       " 2.00  3.16  3.74  3.90  3.29  4.07  4.10  4.10  4.060  3.72  3.71  3.97   \n",
       " 2.50  3.92  4.69  4.86  4.08  5.19  5.23  5.20  4.950  4.82  4.64  4.81   \n",
       " 3.00  3.33  4.02  3.16  3.33  4.12  4.11  4.01  4.210  3.66  3.71  3.80   \n",
       " 3.50  1.46  1.72  1.58  1.41  1.57  1.55  1.37  1.380  1.29  1.20  1.20   \n",
       " 4.00  1.07  1.24  1.12  1.03  1.11  1.23  1.04  1.081  0.95  0.93  0.97   \n",
       " 5.00  0.30  0.36  0.32  0.30  0.28  0.33  0.36  0.380  0.37  0.32  0.36   \n",
       "\n",
       "         11    12  \n",
       "-2.25  1.62  3.44  \n",
       "-1.00  1.27  1.70  \n",
       "-0.50  1.02  0.84  \n",
       " 0.00  1.41  0.96  \n",
       " 0.50  1.82  1.27  \n",
       " 1.00  2.95  2.24  \n",
       " 1.50  3.02  2.23  \n",
       " 2.00  4.82  3.85  \n",
       " 2.50  5.80  4.97  \n",
       " 3.00  4.60  4.15  \n",
       " 3.50  1.39  1.39  \n",
       " 4.00  1.07  1.09  \n",
       " 5.00  0.49  0.42  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd      \n",
    "import numpy as np\n",
    "import sys\n",
    "import SedSAS\n",
    "\n",
    "## list of sieve apertures used in the analysis. We'll use these as dataframe index values\n",
    "Apertures=[-2.25,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,5.0 ]\n",
    "\n",
    "## 1.), read the raw file into a pandas dataframe:\n",
    "file_path='../Sample_Data/'\n",
    "file='Sample_Data.xlsx'\n",
    "df=pd.read_excel(file_path+file, header=1)\n",
    "\n",
    "## 2.) Drop the Sample Name column from the dataframe. We don't need it. Sinceit's in the \n",
    "## first position (aLL rows of the first column, in Python slicing parlance: [:,0:1]):\n",
    "df.drop(df.iloc[:,0:1], axis=1, inplace=True)\n",
    "\n",
    "## 3.) Rename the 'Pan' column to 5:\n",
    "df.rename(columns={'Pan':5}, inplace=True)\n",
    "\n",
    "## (4.) Then, then, we'll transpose the dataframe to change it from sample row into sample \n",
    "## columnar order\n",
    "df=df.T.copy()\n",
    "\n",
    "df\n",
    "## each column in df is now a single sample. each row in df is a single weight observation\n",
    "## in the sample. We'll retrieve the needed aperture data from the dataframe index.\n",
    "\n",
    "## Now, the dataframe's ready to go, so let do the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute grain-size statistical metrics for each sample in the dataframe\n",
    "\n",
    "Iterate through the new dataframe, column by column (sample by sample), passing the sample to SedSAS for processing. Note that SedSAS is designed to handle only a single sample at a time. SedSAS analysis results for each and all samples in the input data file are returned in a new dataframe. This new dataframe can be the basis for continued analysis, or can be written out to a comma-separated values (csv) file--see \"Save results to a Comma-Separated Values (csv) text file\" at the end of this notebook for the details--for archive or in use eslewhere. \n",
    "\n",
    "This script uses a convenience method ComputeGSStats() to generate statistics (mean, sorting, skewness, and kurtosis) using 4 different computational approaches. Included in these appraoches are the logarithmic and geometric inclusive graphics from Folk and Ward (1957) and Folk, (1980) and the geomtric and logartihmic method of moments from Krumbien and Pettijohn,(1938). A fifth approach is represented in SedSAS, computation of the arithmetic method of moments, but as this is considered a much less robust and much less commonly used algorithm, it is omitted from ComputeGSStats(). If desired, the user can alter ComputeGSStats() to include this fifth method. \n",
    "\n",
    "In addition to ComputeGSStats() the user has the option to call methods which compute statistics using the five approaches represented in SedSAS (four of which are built into ComputeGSStats()) individually to customize analyses based on user need and experimental design. See the GitHub readme for more information.\n",
    "\n",
    "Plots (histograms and CDFs) can also be generated for each sample and captured in individual (by sample) Portable Network Graphics (PNG) files. To do this add the following line of PLOTting code inside the for loop struct:    \n",
    "    \n",
    "    ## create a class instance and generate statistics using the convenience method ComputeGSStats():         <-- existing line\n",
    "    out_list.append( ssc.ComputeGSStats() )                                                                   <-- existing line\n",
    "    ssc.PLOTDualSampleWeightPercents(printTo='file')                                                          <-- add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Processing sample:  Sample 0\n",
      "WARNING: percent of coarsest fraction in sample Sample 0 is:\n",
      "10.52 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "\n",
      "\n",
      " Processing sample:  Sample 1\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 2 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 2\n",
      "WARNING: percent of coarsest fraction in sample Sample 2 is:\n",
      "8.15 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 2 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 3\n",
      "WARNING: percent of coarsest fraction in sample Sample 3 is:\n",
      "12.64 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 4\n",
      "WARNING: percent of coarsest fraction in sample Sample 4 is:\n",
      "8.29 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 5\n",
      "WARNING: percent of coarsest fraction in sample Sample 5 is:\n",
      "9.7 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 6\n",
      "WARNING: percent of coarsest fraction in sample Sample 6 is:\n",
      "6.71 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 7\n",
      "WARNING: percent of coarsest fraction in sample Sample 7 is:\n",
      "11.14 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 2 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 8\n",
      "WARNING: percent of coarsest fraction in sample Sample 8 is:\n",
      "17.01 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 16% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 15% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 2 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 9\n",
      "WARNING: percent of coarsest fraction in sample Sample 9 is:\n",
      "9.9 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 10\n",
      "WARNING: percent of coarsest fraction in sample Sample 10 is:\n",
      "15.71 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 15% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 11\n",
      "WARNING: percent of coarsest fraction in sample Sample 11 is:\n",
      "5.18 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 2 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n",
      "\n",
      "\n",
      " Processing sample:  Sample 12\n",
      "WARNING: percent of coarsest fraction in sample Sample 12 is:\n",
      "12.05 percent. This exceeds 5% of total by weight.\n",
      "Values in excess of 5% can introduce significant error in some analyses.\n",
      "\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 5% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 3% quantile < min cum wt%, extrapolating solution\n",
      "WARNING: 10% quantile < min cum wt%, extrapolating solution\n",
      "WARNING! Number of modes in sample is 3 Textural shape descriptions of a\n",
      "multimodal sample distribution are unreliable or possibly even nonsensical.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FWLogKurt</th>\n",
       "      <th>FWLogKurtClass</th>\n",
       "      <th>FWLogMean</th>\n",
       "      <th>FWLogSizeClass</th>\n",
       "      <th>FWLogSkew</th>\n",
       "      <th>FWLogSkewClass</th>\n",
       "      <th>FWLogSort</th>\n",
       "      <th>FWLogSortCLass</th>\n",
       "      <th>McLogKurt</th>\n",
       "      <th>McLogKurtClass</th>\n",
       "      <th>...</th>\n",
       "      <th>MoMLogSkew</th>\n",
       "      <th>MoMLogSkewClass</th>\n",
       "      <th>MoMLogSort</th>\n",
       "      <th>MoMLogSortCLass</th>\n",
       "      <th>PrimaryMode_mm</th>\n",
       "      <th>PrimaryMode_phi</th>\n",
       "      <th>SecondaryMode_mm</th>\n",
       "      <th>SecondaryMode_phi</th>\n",
       "      <th>TertiaryMode_mm</th>\n",
       "      <th>TertiaryMode_phi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.878</td>\n",
       "      <td>Platykurtic</td>\n",
       "      <td>0.911</td>\n",
       "      <td>Coarse Sand</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>2.055</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.964</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.134</td>\n",
       "      <td>Leptokurtic</td>\n",
       "      <td>1.422</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.689</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.671</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.192</td>\n",
       "      <td>Leptokurtic</td>\n",
       "      <td>1.251</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.832</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.798</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850</td>\n",
       "      <td>Platykurtic</td>\n",
       "      <td>0.791</td>\n",
       "      <td>Coarse Sand</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>2.104</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>2.002</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.972</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>1.046</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.905</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.841</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.991</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>1.051</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.929</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.875</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.074</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>1.203</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.814</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.774</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.043</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>1.002</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.961</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.895</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.884</td>\n",
       "      <td>Platykurtic</td>\n",
       "      <td>0.621</td>\n",
       "      <td>Coarse Sand</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>2.221</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>2.054</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>4.756828</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.092</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>1.176</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.836</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.829</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.002</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>0.693</td>\n",
       "      <td>Coarse Sand</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>2.183</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.273</td>\n",
       "      <td>Leptokurtic</td>\n",
       "      <td>1.476</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>1.599</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.603</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.082</td>\n",
       "      <td>Mesokurtic</td>\n",
       "      <td>1.028</td>\n",
       "      <td>Medium Sand</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>Strongly coarse-skewed</td>\n",
       "      <td>2.021</td>\n",
       "      <td>Very poorly sorted</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>Coarse skewed</td>\n",
       "      <td>1.944</td>\n",
       "      <td>Poorly sorted</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.75683</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FWLogKurt FWLogKurtClass  FWLogMean FWLogSizeClass  FWLogSkew  \\\n",
       "0       0.878    Platykurtic      0.911    Coarse Sand     -0.396   \n",
       "1       1.134    Leptokurtic      1.422    Medium Sand     -0.361   \n",
       "2       1.192    Leptokurtic      1.251    Medium Sand     -0.399   \n",
       "3       0.850    Platykurtic      0.791    Coarse Sand     -0.389   \n",
       "4       0.972     Mesokurtic      1.046    Medium Sand     -0.395   \n",
       "5       0.991     Mesokurtic      1.051    Medium Sand     -0.393   \n",
       "6       1.074     Mesokurtic      1.203    Medium Sand     -0.407   \n",
       "7       1.043     Mesokurtic      1.002    Medium Sand     -0.403   \n",
       "8       0.884    Platykurtic      0.621    Coarse Sand     -0.446   \n",
       "9       1.092     Mesokurtic      1.176    Medium Sand     -0.393   \n",
       "10      1.002     Mesokurtic      0.693    Coarse Sand     -0.461   \n",
       "11      1.273    Leptokurtic      1.476    Medium Sand     -0.348   \n",
       "12      1.082     Mesokurtic      1.028    Medium Sand     -0.454   \n",
       "\n",
       "            FWLogSkewClass  FWLogSort      FWLogSortCLass McLogKurt  \\\n",
       "0   Strongly coarse-skewed      2.055  Very poorly sorted         -   \n",
       "1   Strongly coarse-skewed      1.689       Poorly sorted         -   \n",
       "2   Strongly coarse-skewed      1.832       Poorly sorted         -   \n",
       "3   Strongly coarse-skewed      2.104  Very poorly sorted         -   \n",
       "4   Strongly coarse-skewed      1.905       Poorly sorted         -   \n",
       "5   Strongly coarse-skewed      1.929       Poorly sorted         -   \n",
       "6   Strongly coarse-skewed      1.814       Poorly sorted         -   \n",
       "7   Strongly coarse-skewed      1.961       Poorly sorted         -   \n",
       "8   Strongly coarse-skewed      2.221  Very poorly sorted         -   \n",
       "9   Strongly coarse-skewed      1.836       Poorly sorted         -   \n",
       "10  Strongly coarse-skewed      2.183  Very poorly sorted         -   \n",
       "11  Strongly coarse-skewed      1.599       Poorly sorted         -   \n",
       "12  Strongly coarse-skewed      2.021  Very poorly sorted         -   \n",
       "\n",
       "   McLogKurtClass       ...         MoMLogSkew MoMLogSkewClass MoMLogSort  \\\n",
       "0               -       ...             -0.552   Coarse skewed      1.964   \n",
       "1               -       ...             -0.797   Coarse skewed      1.671   \n",
       "2               -       ...             -0.823   Coarse skewed      1.798   \n",
       "3               -       ...             -0.516   Coarse skewed      2.002   \n",
       "4               -       ...             -0.651   Coarse skewed      1.841   \n",
       "5               -       ...             -0.663   Coarse skewed      1.875   \n",
       "6               -       ...             -0.750   Coarse skewed      1.774   \n",
       "7               -       ...             -0.719   Coarse skewed      1.895   \n",
       "8               -       ...             -0.596   Coarse skewed      2.054   \n",
       "9               -       ...             -0.772   Coarse skewed      1.829   \n",
       "10              -       ...             -0.692   Coarse skewed      2.000   \n",
       "11              -       ...             -0.913   Coarse skewed      1.603   \n",
       "12              -       ...             -0.776   Coarse skewed      1.944   \n",
       "\n",
       "       MoMLogSortCLass  PrimaryMode_mm PrimaryMode_phi  SecondaryMode_mm  \\\n",
       "0        Poorly sorted        0.176777            2.50                 -   \n",
       "1        Poorly sorted        0.176777            2.50                 2   \n",
       "2        Poorly sorted        0.176777            2.50           4.75683   \n",
       "3   Very poorly sorted        0.176777            2.50           4.75683   \n",
       "4        Poorly sorted        0.176777            2.50                 2   \n",
       "5        Poorly sorted        0.176777            2.50           4.75683   \n",
       "6        Poorly sorted        0.176777            2.50                 2   \n",
       "7        Poorly sorted        0.176777            2.50           4.75683   \n",
       "8   Very poorly sorted        4.756828           -2.25          0.176777   \n",
       "9        Poorly sorted        0.176777            2.50           4.75683   \n",
       "10  Very poorly sorted        0.176777            2.50           4.75683   \n",
       "11       Poorly sorted        0.176777            2.50           4.75683   \n",
       "12       Poorly sorted        0.176777            2.50           4.75683   \n",
       "\n",
       "   SecondaryMode_phi  TertiaryMode_mm TertiaryMode_phi  \n",
       "0                  -                -                -  \n",
       "1                 -1                -                -  \n",
       "2              -2.25                -                -  \n",
       "3              -2.25              0.5                1  \n",
       "4                 -1              0.5                1  \n",
       "5              -2.25              0.5                1  \n",
       "6                 -1              0.5                1  \n",
       "7              -2.25                -                -  \n",
       "8                2.5                -                -  \n",
       "9              -2.25              0.5                1  \n",
       "10             -2.25              0.5                1  \n",
       "11             -2.25                -                -  \n",
       "12             -2.25              0.5                1  \n",
       "\n",
       "[13 rows x 30 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set a counter variable to be used to create and pass a unique identifier to SedSAS for each sample\n",
    "c=0\n",
    "\n",
    "out_list=[]        # a temporary Python list to hold the dictionary of results returned for each sample in df\n",
    "\n",
    "## loop thru the dataframe df, reading it column by column (by sample) and creating dataframe df_\n",
    "## anew with each iteration. df_ will contain, for each iteration, an aperture column and a weight\n",
    "## column. The latter is unique to the sample. This is passed to the SedSAS initializer.\n",
    "\n",
    "for sample in df:\n",
    "    df_= df[sample].to_frame()    # convert the single column from a series to a dataframe called df_\n",
    "    df_.columns=['Weight']        # label the single column 'Weight'\n",
    "    df_['Aperture']=df_.index     # copy the dataframe index to a new column 'Aperture'\n",
    "    df_.set_index( [np.arange(0,len(df_['Aperture']),1)], inplace=True  )  # reset index to sequential numbers\n",
    "    \n",
    "    ID = 'Sample '+str(c)          # create an arbitrary unique idenifier for the current sample\n",
    "    c=c+1\n",
    "    \n",
    "    ## create a class instance and generate statistics using the convenience method ComputeGSStats().\n",
    "    ## ComputeGSStats() returns results in a Python dictionary that is then appended onto out_list\n",
    "    ssc = SedSAS.SedSAS(df_, ID )\n",
    "    out_list.append( ssc.ComputeGSStats() )\n",
    " \n",
    "## create the output dataframe and write to screen:\n",
    "df_out=pd.DataFrame(out_list).replace(np.nan, value='-')\n",
    "df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results to a Comma-Separated Values (csv) text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saves the contents of the df_out dataframe to a csv file:\n",
    "df_out.to_csv('./multi_sample_results.csv')\n",
    "\n",
    "## the file is written to the local subdirectory. You can change this by modifyng the \n",
    "## path string in the .to_csv directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
